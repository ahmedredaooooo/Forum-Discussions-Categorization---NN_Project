{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs_Work\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of strings in 'Discussion' column: 128\n"
     ]
    }
   ],
   "source": [
    "# Dataset Preparation\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Map categories to integers\n",
    "category_mapping = {'Politics': 0, 'Sports': 1, 'Media': 2, 'Market & Economy': 3, 'STEM': 4}\n",
    "data['Category'] = data['Category'].replace(category_mapping)\n",
    "\n",
    "\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "print(\"Maximum number of strings in 'Discussion' column:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langdetect import detect, DetectorFactory\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set seed for consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Map categories to integers\n",
    "category_mapping = {'Politics': 0, 'Sports': 1, 'Media': 2, 'Market & Economy': 3, 'STEM': 4}\n",
    "data['Category'] = data['Category'].replace(category_mapping)\n",
    "\n",
    "# Remove rows with NaN or empty strings in 'Discussion'\n",
    "data.dropna(subset=['Discussion'], inplace=True)  # Remove NaN values\n",
    "data['Discussion'] = data['Discussion'].astype(str)  # Ensure all values are strings\n",
    "data = data[data['Discussion'].str.strip() != '']  # Remove empty or whitespace-only strings\n",
    "\n",
    "# Define a safe language detection function\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False  # Return False for texts that cause an exception\n",
    "\n",
    "# Filter rows where 'Discussion' is detected as English\n",
    "data = data[data['Discussion'].apply(is_english)]\n",
    "\n",
    "# Remove short discussions (noise) based on word count\n",
    "min_word_count = 3  # Minimum number of words\n",
    "data = data[data['Discussion'].apply(lambda x: len(x.split()) >= min_word_count)]\n",
    "\n",
    "# Normalize text\n",
    "data['Discussion'] = data['Discussion'].str.lower()\n",
    "data['Discussion'] = data['Discussion'].apply(lambda x: re.sub(r'http[s]?://\\S+', '', x))  # Remove URLs\n",
    "data['Discussion'] = data['Discussion'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))  # Remove non-alphanumeric\n",
    "\n",
    "# Tokenize, remove stop words, and lemmatize\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "data['Discussion'] = data['Discussion'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['Discussion'], data['Category'], test_size=0.25, stratify=data['Category'], shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "max_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs_Work\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def tokenize_texts(texts, tokenizer, max_length=max_length):\n",
    "    tokenized = tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return tokenized['input_ids']\n",
    "\n",
    "X_train_tokenized = tokenize_texts(X_train, tokenizer, max_length=max_length)\n",
    "X_test_tokenized = tokenize_texts(X_test, tokenizer, max_length=max_length)\n",
    "\n",
    "# One-hot encode labels\n",
    "num_classes = len(category_mapping)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16840, 128), dtype=int32, numpy=\n",
       "array([[  101,  2215, 10463, ...,     0,     0,     0],\n",
       "       [  101,  3160,  2691, ...,     0,     0,     0],\n",
       "       [  101,  2755,  2438, ...,  2008,  2072,   102],\n",
       "       ...,\n",
       "       [  101,  2377,  6708, ...,     0,     0,     0],\n",
       "       [  101, 10047,  2469, ...,     0,     0,     0],\n",
       "       [  101,  2947,  4824, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Positional Encoding Function\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Apply sin to even indices\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Apply cos to odd indices\n",
    "    return tf.cast(angle_rads, dtype=tf.float32)\n",
    "\n",
    "# Transformer Encoder Block\n",
    "def transformer_encoder(inputs, d_model, num_heads, ffn_units, dropout_rate, l2_reg):\n",
    "    # Multi-head self-attention\n",
    "    attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(inputs, inputs)\n",
    "    attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = layers.LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "\n",
    "    # Feedforward network\n",
    "    ffn = layers.Dense(ffn_units, activation=\"relu\", kernel_regularizer=l2(l2_reg))(attention_output)\n",
    "    ffn = layers.Dense(d_model, kernel_regularizer=l2(l2_reg))(ffn)\n",
    "    ffn_output = layers.Dropout(dropout_rate)(ffn)\n",
    "    ffn_output = layers.LayerNormalization(epsilon=l2_reg)(attention_output + ffn_output)\n",
    "\n",
    "    return ffn_output\n",
    "\n",
    "# Transformer Model\n",
    "def build_transformer_model(vocab_size, max_length, d_model, num_heads, ffn_units, num_classes, dropout_rate, l2_reg, num_layers):\n",
    "    inputs = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"inputs\")\n",
    "    \n",
    "    # Embedding Layer\n",
    "    embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "    embeddings = embedding_layer(inputs)  # Shape: [batch_size, max_length, d_model]\n",
    "    \n",
    "    # Positional Encoding\n",
    "    position_encodings = positional_encoding(max_length, d_model)\n",
    "    embeddings += position_encodings  # Add positional encoding\n",
    "    \n",
    "    # Stack multiple transformer encoder layers\n",
    "    x = embeddings\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, d_model, num_heads, ffn_units, dropout_rate, l2_reg)\n",
    "    \n",
    "    # Global Average Pooling and Output\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=l2(l2_reg))(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"Custom_Transformer\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before Reinitialization ---\n",
      "Layer: dense_20\n",
      "Kernel Mean: -0.0027202710043638945, Std: 0.13395832479000092\n",
      "Bias Mean: 0.0, Std: 0.0\n",
      "Layer: dense_21\n",
      "Kernel Mean: -0.0002069069305434823, Std: 0.1332174390554428\n",
      "Bias Mean: 0.0, Std: 0.0\n",
      "Layer: dense_22\n",
      "Kernel Mean: -0.0028308317996561527, Std: 0.1948351114988327\n",
      "Bias Mean: 0.0, Std: 0.0\n",
      "--- After Reinitialization ---\n",
      "Layer: dense_20\n",
      "Kernel Mean: 0.0043540578335523605, Std: 0.13398945331573486\n",
      "Bias Mean: 0.0, Std: 0.0\n",
      "Layer: dense_21\n",
      "Kernel Mean: 0.0008082825806923211, Std: 0.13301114737987518\n",
      "Bias Mean: 0.0, Std: 0.0\n",
      "Layer: dense_22\n",
      "Kernel Mean: 0.014705296605825424, Std: 0.19281302392482758\n",
      "Bias Mean: 0.0, Std: 0.0\n",
      "Weights were reinitialized: True\n",
      "Model: \"Custom_Transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 128, 48)      480000      inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_20 (TFOpLa (None, 128, 48)      0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_7 (MultiHe (None, 128, 48)      337008      tf.__operators__.add_20[0][0]    \n",
      "                                                                 tf.__operators__.add_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 128, 48)      0           multi_head_attention_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_21 (TFOpLa (None, 128, 48)      0           tf.__operators__.add_20[0][0]    \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 128, 48)      96          tf.__operators__.add_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 128, 64)      3136        layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 128, 48)      3120        dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 128, 48)      0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_22 (TFOpLa (None, 128, 48)      0           layer_normalization_14[0][0]     \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 128, 48)      96          tf.__operators__.add_22[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 48)           0           layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 48)           0           global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 5)            245         dropout_22[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 823,701\n",
      "Trainable params: 823,701\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters\n",
    "dropout_rate = 0.5\n",
    "l2_reg = 1e-7\n",
    "num_layers = 1\n",
    "\n",
    "\n",
    "d_model = 48  # Set the embedding size\n",
    "num_heads = 36\n",
    "ffn_units = 64\n",
    "\n",
    "num_classes = 5\n",
    "vocab_size = 10000  # Assume a vocab size of 10,000 for this example\n",
    "\n",
    "# Learning rate scheduler\n",
    "learning_rate_schedule = ExponentialDecay(initial_learning_rate=1e-4, decay_steps=1000, decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "# Function to reinitialize weights\n",
    "def reinitialize_weights(model):\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel_initializer') and hasattr(layer, 'bias_initializer'):\n",
    "            layer.kernel.assign(layer.kernel_initializer(tf.shape(layer.kernel)))\n",
    "            layer.bias.assign(layer.bias_initializer(tf.shape(layer.bias)))\n",
    "        if hasattr(layer, 'gamma_initializer') and hasattr(layer, 'beta_initializer'):\n",
    "            if layer.gamma is not None:\n",
    "                layer.gamma.assign(layer.gamma_initializer(tf.shape(layer.gamma)))\n",
    "            if layer.beta is not None:\n",
    "                layer.beta.assign(layer.beta_initializer(tf.shape(layer.beta)))\n",
    "\n",
    "# Build the model\n",
    "custom_transformer_model = build_transformer_model(\n",
    "    vocab_size, max_length, d_model, num_heads, ffn_units, num_classes, dropout_rate, l2_reg, num_layers\n",
    ")\n",
    "\n",
    "weights_before = [layer.get_weights() for layer in custom_transformer_model.layers]\n",
    "\n",
    "# Reinitialize weights (optional, useful if weights were loaded previously)\n",
    "def print_weights_summary(model, message):\n",
    "    print(f\"--- {message} ---\")\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel'):\n",
    "            print(f\"Layer: {layer.name}\")\n",
    "            print(f\"Kernel Mean: {tf.reduce_mean(layer.kernel).numpy()}, Std: {tf.math.reduce_std(layer.kernel).numpy()}\")\n",
    "        if hasattr(layer, 'bias'):\n",
    "            print(f\"Bias Mean: {tf.reduce_mean(layer.bias).numpy()}, Std: {tf.math.reduce_std(layer.bias).numpy()}\")\n",
    "print_weights_summary(custom_transformer_model, \"Before Reinitialization\")\n",
    "reinitialize_weights(custom_transformer_model)\n",
    "print_weights_summary(custom_transformer_model, \"After Reinitialization\")\n",
    "weights_after = [layer.get_weights() for layer in custom_transformer_model.layers]\n",
    "\n",
    "\n",
    "# Check if weights are different\n",
    "weights_changed = any(\n",
    "    not all(\n",
    "        tf.reduce_all(tf.equal(w1, w2)).numpy()\n",
    "        for w1, w2 in zip(layer_before, layer_after)\n",
    "    )\n",
    "    for layer_before, layer_after in zip(weights_before, weights_after)\n",
    ")\n",
    "print(\"Weights were reinitialized:\", weights_changed)\n",
    "\n",
    "# Compile the model\n",
    "custom_transformer_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print model summary\n",
    "custom_transformer_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Build the model\n",
    "# custom_transformer_model = build_transformer_model(\n",
    "#     vocab_size, max_length, d_model, num_heads, ffn_units, num_classes, dropout_rate, l2_reg, num_layers\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # Compile the model\n",
    "# custom_transformer_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# # Print model summary\n",
    "# custom_transformer_model.summary()\n",
    "\n",
    "# # Early stopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1053/1053 [==============================] - 22s 20ms/step - loss: 1.6374 - accuracy: 0.2184 - val_loss: 1.5938 - val_accuracy: 0.2408\n",
      "Epoch 2/20\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 1.6119 - accuracy: 0.2235 - val_loss: 1.5768 - val_accuracy: 0.2380\n",
      "Epoch 3/20\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 1.5880 - accuracy: 0.2572 - val_loss: 1.5608 - val_accuracy: 0.2472\n",
      "Epoch 4/20\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 1.5050 - accuracy: 0.3393 - val_loss: 1.3002 - val_accuracy: 0.5080\n",
      "Epoch 5/20\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 1.2044 - accuracy: 0.5235 - val_loss: 1.2689 - val_accuracy: 0.4827\n",
      "Epoch 6/20\n",
      "1053/1053 [==============================] - 29s 27ms/step - loss: 1.0339 - accuracy: 0.5885 - val_loss: 1.1091 - val_accuracy: 0.5771\n",
      "Epoch 7/20\n",
      "1053/1053 [==============================] - 29s 27ms/step - loss: 0.9667 - accuracy: 0.6198 - val_loss: 1.0811 - val_accuracy: 0.5848\n",
      "Epoch 8/20\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.9172 - accuracy: 0.6425 - val_loss: 1.0663 - val_accuracy: 0.6030\n",
      "Epoch 9/20\n",
      "1053/1053 [==============================] - 29s 27ms/step - loss: 0.8823 - accuracy: 0.6624 - val_loss: 1.0842 - val_accuracy: 0.6005\n",
      "Epoch 10/20\n",
      "1053/1053 [==============================] - 28s 26ms/step - loss: 0.8435 - accuracy: 0.6804 - val_loss: 1.0712 - val_accuracy: 0.6124\n",
      "Epoch 11/20\n",
      "1053/1053 [==============================] - 29s 28ms/step - loss: 0.8164 - accuracy: 0.6961 - val_loss: 1.0756 - val_accuracy: 0.6170\n",
      "Epoch 12/20\n",
      "1053/1053 [==============================] - 30s 29ms/step - loss: 0.7884 - accuracy: 0.7122 - val_loss: 1.0720 - val_accuracy: 0.6261\n",
      "Epoch 13/20\n",
      "1053/1053 [==============================] - 30s 28ms/step - loss: 0.7649 - accuracy: 0.7191 - val_loss: 1.0904 - val_accuracy: 0.6252\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = custom_transformer_model.fit(\n",
    "    X_train_tokenized.numpy(),\n",
    "    y_train,\n",
    "    validation_data=(X_test_tokenized.numpy(), y_test),\n",
    "    batch_size=16,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
